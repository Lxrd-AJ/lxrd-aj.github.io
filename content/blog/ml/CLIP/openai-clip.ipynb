{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP (Contrastive Language-Image Pretraining)\n",
    "\n",
    "A neural network that learns visual concepts from natural language supervision that can achieve zero-shot classification on new benchmarks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources Used\n",
    "- [ ] https://openai.com/research/clip \n",
    "    - [ ] https://arxiv.org/abs/2103.00020\n",
    "    - [ ] https://github.com/openai/CLIP/tree/main\n",
    "- [ ] https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2 \n",
    "- [ ] https://github.com/mlfoundations/open_clip/tree/main\n",
    "- [ ] https://huggingface.co/docs/transformers/model_doc/clip\n",
    "\n",
    "## Datasets\n",
    "* https://pytorch.org/vision/main/generated/torchvision.datasets.Flickr8k.html \n",
    "* https://unsplash.com/data "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
